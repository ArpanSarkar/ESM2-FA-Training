model:
  # Pretrained checkpoint to evaluate
  model_name: "facebook/esm2_t33_650M_UR50D"
  max_position_embeddings: 1026
  max_batch_size: 8192
  use_fa: true
  hidden_dropout_prob: 0.0
  attention_probs_dropout_prob: 0.0

training:
  # Evaluation-only run
  eval_only: true

  # Keep the same accumulation as main config
  gradient_accumulation_steps: 32

  # Optimizer/lr fields are unused in eval_only but kept for completeness
  learning_rate: 4e-4
  optimizer:
    beta_1: 0.9
    beta_2: 0.95
    epsilon: 1e-8
    weight_decay: 0.01

  gradient_clipping: 0.5
  lr_scheduler: "cosine"
  schedule_steps: 100000
  max_steps: 200000
  warmup_steps: 2500

  mlm_probability: 0.15
  logging_steps: 1
  eval_steps: 1000
  save_steps: 1000
  save_total_limit: 2

  output_dir: "./runs/esm2_650m_eval_ref"

  load_best_model_at_end: false
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  early_stopping:
    enabled: false
    patience: 3
    threshold: 0.0

  mixed_precision: "bf16"
  dataloader_num_workers: 4
  dataloader_prefetch_factor: 4
  dataloader_pin_memory: true
  seed: 100

data:
  chunk_size: 100000
  default_shard_size: 25000

